version: "3.8"

services:
  frontend:
    build:
      context: .
      dockerfile: docker/Dockerfile.frontend
    container_name: ollama-chat-frontend
    ports:
      - "3000:80"
    volumes:
      - ./build/frontend:/usr/share/nginx/html:ro
    networks:
      - ollama-chat-network
    restart: unless-stopped
    depends_on:
      - backend

  backend:
    build:
      context: .
      dockerfile: docker/Dockerfile.backend
    container_name: ollama-chat-backend
    ports:
      - "8082:8080"
    volumes:
      - ./src/backend:/app:ro
      - ./data:/data
    environment:
      NODE_ENV: production
      OLLAMA_HOST: http://ollama:11434
      DB_PATH: /data/ollama-chat.db
    networks:
      - ollama-chat-network
    depends_on:
      - ollama
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: "0.5"

  ollama:
    image: ollama/ollama:latest
    container_name: ollama-chat-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama-models:/root/.ollama
    networks:
      - ollama-chat-network
    restart: unless-stopped

networks:
  ollama-chat-network:
    driver: bridge

volumes:
  ollama-models:
    driver: local
