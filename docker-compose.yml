version: "3.8"

services:
  frontend:
    build:
      context: .
      dockerfile: docker/Dockerfile.frontend
    container_name: ollama-chat-frontend
    ports:
      - "3000:80"
    environment:
      - OLLAMA_API_BASE=${OLLAMA_API_BASE:-http://host.docker.internal:11434}
      - BACKEND_API_BASE=${BACKEND_API_BASE:-http://localhost:8082}
    volumes:
      - ./build/frontend:/usr/share/nginx/html
    networks:
      - ollama-chat-network
    restart: unless-stopped
    depends_on:
      - backend

  backend:
    build:
      context: .
      dockerfile: docker/Dockerfile.backend
    container_name: ollama-chat-backend
    ports:
      - "8082:8080"
    volumes:
      - ./data:/data
    environment:
      NODE_ENV: production
      OLLAMA_HOST: http://host.docker.internal:11434
      DB_PATH: /data/ollama-chat.db
    networks:
      - ollama-chat-network
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: "0.5"

networks:
  ollama-chat-network:
    driver: bridge

volumes: {}
